{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch.backends import cudnn\n",
    "from torch.backends import cuda\n",
    "from torch.cuda import amp\n",
    "\n",
    "import timm\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "from timm import create_model\n",
    "\n",
    "import wandb\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CN()\n",
    "\n",
    "cfg.PROJECT = 'AIdea fall competition'\n",
    "cfg.NAME = 'ConvNeXt_base'\n",
    "\n",
    "cfg.SEED = 1224\n",
    "cfg.USE_WANDB = False\n",
    "cfg.WANDB_CONT = False\n",
    "cfg.WANDB_ARTIFACT = False\n",
    "\n",
    "cfg.SAVE_DIR = 'ConvNeXt_base' #checkpoint and best weight's output dir\n",
    "\n",
    "cfg.DATA = CN()\n",
    "cfg.DATA.DATASET = 'AIdea fall competition'\n",
    "cfg.DATA.TRAIN_DIR = '../dataset/train' #train data dir\n",
    "cfg.DATA.PUBLIC_TEST_DIR = '../dataset/public_test' # public test data dir\n",
    "cfg.DATA.PRIVATE_TEST_DIR = '../dataset/p_test' # private test data dir\n",
    "cfg.DATA.PSEUDO_DIR = '../dataset/pseudo_data' #None for no pseudo label\n",
    "cfg.DATA.TARGET_CSV = '../dataset/target_merge.csv' #with public private and train coordinate data's csv\n",
    "cfg.DATA.BATCH_SIZE = 8 #depend on gpu memory size\n",
    "cfg.DATA.IMG_SIZE = 768\n",
    "cfg.DATA.PIN_MEMORY = True\n",
    "cfg.DATA.NUM_WORKERS = 8 #depend on cpu core count\n",
    "cfg.DATA.CROP = True\n",
    "cfg.KFOLD_NUM = 0\n",
    "\n",
    "cfg.AUG = CN()\n",
    "cfg.AUG.MIXUP_ALPHA = 0.8\n",
    "cfg.AUG.CUTMIX_ALPHA = 1.0\n",
    "cfg.AUG.CUTMIX_MINMAX = None\n",
    "cfg.AUG.MIXUP_PROB = 0.5\n",
    "cfg.AUG.MIXUP_SWITCH_PROB = 0.5\n",
    "cfg.AUG.MIXUP_MODE = 'batch'\n",
    "cfg.AUG.LABEL_SMOOTHING = 0.1\n",
    "\n",
    "cfg.MODEL = CN()\n",
    "cfg.MODEL.BACKBONE = 'convnext_base_in22k'\n",
    "cfg.MODEL.NUM_CLASSES = 33\n",
    "cfg.MODEL.PRETRAINED = True\n",
    "cfg.MODEL.USE_EMA = True\n",
    "\n",
    "cfg.TRAIN = CN()\n",
    "cfg.TRAIN.EPOCHS = 10 #pseudo train: 5\n",
    "cfg.TRAIN.LR = 1e-4 #pseudo train: 5e-5\n",
    "cfg.TRAIN.ACCUMULATION_STEPS = 8\n",
    "cfg.TRAIN.AMP_ENABLE = True\n",
    "\n",
    "cfg.TRAIN.PRETRAINED = None #pseudo train: choose the best acc@1's checkpoint in first train and assign it\n",
    "cfg.TRAIN.DEL_PRETRAINED = None\n",
    "cfg.TRAIN.USE_CHECKPOINT = False\n",
    "cfg.TRAIN.NEW_RUN = False\n",
    "\n",
    "cfg.TRAIN.OPTIMIZER = CN()\n",
    "cfg.TRAIN.OPTIMIZER.NAME = 'AdamW' #SGD #Adam\n",
    "cfg.TRAIN.OPTIMIZER.WEIGHT_DECAY = 1e-8\n",
    "cfg.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999) #AdamW only\n",
    "#cfg.TRAIN.OPTIMIZER.MOMENTUM = 0.9 #SGD only\n",
    "#cfg.TRAIN.OPTIMIZER.NESTEROV = True #SGD only\n",
    "\n",
    "cfg.TRAIN.LR_SCHEDULER = CN()\n",
    "cfg.TRAIN.LR_SCHEDULER.NAME = 'CosineAnnealingLR'\n",
    "cfg.TRAIN.LR_SCHEDULER.T_MAX = 10 #pseudo train: 5\n",
    "cfg.TRAIN.LR_SCHEDULER.ETA_MIN = 1e-6\n",
    "\n",
    "cfg.VALID = CN()\n",
    "cfg.VALID.MODEL_EMA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "amp_enable = cfg.TRAIN.AMP_ENABLE\n",
    "\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True \n",
    "\n",
    "cuda.matmul.allow_tf32 = True\n",
    "cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = cfg.SEED\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.USE_WANDB:\n",
    "    wandb.init(project=cfg.PROJECT, name=cfg.NAME, config=cfg, resume=cfg.WANDB_CONT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderlist = ['asparagus', 'bambooshoots', 'betel', 'broccoli', 'cauliflower', 'chinesecabbage', 'chinesechives', 'custardapple', 'grape', 'greenhouse', 'greenonion', 'kale', 'lemon', 'lettuce', 'litchi', 'longan', 'loofah', 'mango', 'onion', 'others', 'papaya', 'passionfruit', 'pear', 'pennisetum', 'redbeans', 'roseapple', 'sesbania', 'soybeans', 'sunhemp', 'sweetpotato', 'taro', 'tea', 'waterbamboo']\n",
    "class_list = sorted(folderlist)\n",
    "\n",
    "target_df = pd.read_csv(cfg.DATA.TARGET_CSV)\n",
    "\n",
    "train_images = []\n",
    "for folder in folderlist:\n",
    "    train_images += glob.glob(os.path.join('{}/{}'.format(cfg.DATA.TRAIN_DIR, folder), \"*\"))\n",
    "\n",
    "train_images = sorted(train_images)\n",
    "train_labels = [os.path.split(path)[0].split('/')[-1] for path in train_images]\n",
    "    \n",
    "print(f'Size of dataset: {len(train_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.DATA.PSEUDO_DIR is not None:\n",
    "    pseudo_images = []\n",
    "\n",
    "    for folder in folderlist:\n",
    "        pseudo_images += glob.glob(os.path.join('{}/{}'.format(cfg.DATA.PSEUDO_DIR, folder), \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = []\n",
    "val_folds = []\n",
    "\n",
    "sfolder = StratifiedKFold(n_splits=25, random_state=cfg.SEED, shuffle=True)\n",
    "\n",
    "for train_idx, val_idx in sfolder.split(train_images, train_labels):\n",
    "    train_folds.append(train_idx)\n",
    "    val_folds.append(val_idx)\n",
    "\n",
    "train_list = [train_images[idx] for idx in train_folds[cfg.KFOLD_NUM]]\n",
    "valid_list = [train_images[idx] for idx in val_folds[cfg.KFOLD_NUM]]\n",
    "\n",
    "train_list = train_list + pseudo_images\n",
    "\n",
    "print('Number of training Data: {}'.format(len(train_list)))\n",
    "print('Number of validation Data: {}'.format(len(valid_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform =  A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, border_mode=0, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Resize(cfg.DATA.IMG_SIZE, cfg.DATA.IMG_SIZE),\n",
    "    A.Cutout(max_h_size=int(cfg.DATA.IMG_SIZE * 0.25), max_w_size=int(cfg.DATA.IMG_SIZE * 0.25), num_holes=1, p=0.25),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(cfg.DATA.IMG_SIZE, cfg.DATA.IMG_SIZE),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=cfg.AUG.MIXUP_ALPHA,\n",
    "    cutmix_alpha=cfg.AUG.CUTMIX_ALPHA, \n",
    "    cutmix_minmax=cfg.AUG.CUTMIX_MINMAX,\n",
    "    prob=cfg.AUG.MIXUP_PROB, \n",
    "    switch_prob=cfg.AUG.MIXUP_SWITCH_PROB, \n",
    "    mode=cfg.AUG.MIXUP_MODE,\n",
    "    label_smoothing=cfg.AUG.LABEL_SMOOTHING, \n",
    "    num_classes=cfg.MODEL.NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIdeaDataset(Dataset):\n",
    "    def __init__(self, file_list, class_list, transform, crop=False):\n",
    "        self.file_list = file_list\n",
    "        self.class_list = class_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_list[idx]).copy()\n",
    "        W, H = img.size\n",
    "        \n",
    "        img = np.array(img)\n",
    "\n",
    "        if cfg.DATA.CROP:\n",
    "            target_x = float(target_df.loc[target_df['filename'] == os.path.split(self.file_list[idx])[1]]['target_x'].to_string(index=False))\n",
    "            target_y = float(target_df.loc[target_df['filename'] == os.path.split(self.file_list[idx])[1]]['target_y'].to_string(index=False))\n",
    "            \n",
    "            if W > H:\n",
    "                if W/2 - H/2 + target_x * W < 0:\n",
    "                    x_min = 0\n",
    "                elif W/2 - H/2 + target_x * W > W - H:\n",
    "                    x_min = int(W - H)\n",
    "                else:\n",
    "                    x_min = int(W/2 - H/2 + target_x * W)\n",
    "                x_max = x_min + H\n",
    "                y_min = 0\n",
    "                y_max = H\n",
    "            else:\n",
    "                x_min = 0\n",
    "                x_max = W\n",
    "                if H/2 - W/2 + target_y * H < 0:\n",
    "                    y_min = 0\n",
    "                elif H/2 - W/2 + target_y * H > H - W:\n",
    "                    y_min = int(H - W)\n",
    "                else:\n",
    "                    y_min = int(H/2 - W/2 + target_y * H)\n",
    "                y_max = y_min + W\n",
    "            \n",
    "            img_crop = A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max)(image=img)['image']\n",
    "        else:\n",
    "            img_crop = img\n",
    "        \n",
    "        img_transformed = self.transform(image=img_crop)\n",
    "        img_transformed = img_transformed['image']\n",
    "        \n",
    "        label_name = os.path.split(self.file_list[idx])[0].split('/')[-1]\n",
    "        label = self.class_list.index(label_name)\n",
    "        \n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AIdeaDataset(train_list, class_list, transform=train_transform)\n",
    "valid_data = AIdeaDataset(valid_list, class_list, transform=valid_transform)\n",
    "query_data = AIdeaDataset(train_list, class_list, transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=cfg.DATA.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=cfg.DATA.PIN_MEMORY,\n",
    "    num_workers=cfg.DATA.NUM_WORKERS,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_data,\n",
    "    batch_size=cfg.DATA.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=cfg.DATA.PIN_MEMORY,\n",
    "    num_workers=cfg.DATA.NUM_WORKERS,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    dataset=query_data,\n",
    "    batch_size=cfg.DATA.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=cfg.DATA.PIN_MEMORY,\n",
    "    num_workers=cfg.DATA.NUM_WORKERS,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = gem(x, p=self.p, eps=self.eps)   \n",
    "        return ret\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.n_classes = cfg.MODEL.NUM_CLASSES\n",
    "        \n",
    "        self.backbone = timm.create_model(cfg.MODEL.BACKBONE, \n",
    "                                          pretrained=True, \n",
    "                                          num_classes=33,\n",
    "                                          drop_path_rate=0.2,\n",
    "                                          head_init_scale=0.001)\n",
    "        \n",
    "        self.backbone.head.global_pool = GeM(p_trainable=True)\n",
    "\n",
    "    def forward(self, img): \n",
    "        x = self.backbone.forward_features(img)\n",
    "        \n",
    "        logits = self.backbone.forward_head(x)\n",
    "        x_emb = self.backbone.forward_head(x, pre_logits=True)\n",
    "        \n",
    "        return logits, x_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cuda()\n",
    "if cfg.MODEL.USE_EMA:\n",
    "    model_ema = timm.utils.ModelEmaV2(model, decay=0.999, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion_STCE = SoftTargetCrossEntropy()\n",
    "criterion_CE = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "if cfg.TRAIN.OPTIMIZER.NAME is 'Adam':\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=cfg.TRAIN.LR,\n",
    "        weight_decay=cfg.TRAIN.OPTIMIZER.WEIGHT_DECAY,\n",
    "    )\n",
    "elif cfg.TRAIN.OPTIMIZER.NAME is 'SGD':\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=cfg.TRAIN.LR,\n",
    "        nesterov=cfg.TRAIN.OPTIMIZER.NESTEROV,\n",
    "        momentum=cfg.TRAIN.OPTIMIZER.MOMENTUM,\n",
    "        weight_decay=cfg.TRAIN.OPTIMIZER.WEIGHT_DECAY,\n",
    "    )\n",
    "elif cfg.TRAIN.OPTIMIZER.NAME is 'AdamW':\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=cfg.TRAIN.LR,\n",
    "        weight_decay=cfg.TRAIN.OPTIMIZER.WEIGHT_DECAY,\n",
    "        betas=cfg.TRAIN.OPTIMIZER.BETAS,\n",
    "    )\n",
    "else:\n",
    "    print('WARNING: No optimizer chosen')\n",
    "\n",
    "# scheduler\n",
    "if cfg.TRAIN.LR_SCHEDULER.NAME is 'CosineAnnealingLR':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer=optimizer,\n",
    "        T_max=cfg.TRAIN.LR_SCHEDULER.T_MAX,\n",
    "        eta_min=cfg.TRAIN.LR_SCHEDULER.ETA_MIN,\n",
    "    )\n",
    "elif cfg.TRAIN.LR_SCHEDULER.NAME is 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    \n",
    "# gradscaler\n",
    "scaler = amp.GradScaler(enabled=amp_enable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(model, pretrained_dir=None):\n",
    "    pretrained = torch.load(pretrained_dir, map_location='cpu')\n",
    "\n",
    "    if cfg.TRAIN.DEL_PRETRAINED is not None:\n",
    "        del_list = [word for word in pretrained['model_state_dict'].keys() if cfg.MODEL.DEL_PRETRAINED  in word]\n",
    "        for key in del_list:\n",
    "            del pretrained['model_state_dict'][key]\n",
    "        strict = False\n",
    "    else:\n",
    "        strict = True\n",
    "    \n",
    "    model.load_state_dict(pretrained['model_state_dict'], strict=strict)\n",
    "    print('Loaded pretrained: {}\\n'.format(pretrained_dir))\n",
    "\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None, specific_checkpoint=None):\n",
    "    last_epochs = 0\n",
    "    best_acc = 0.0\n",
    "    save_dir = cfg.SAVE_DIR\n",
    "    \n",
    "    checkpoint_list = []\n",
    "    \n",
    "    checkpoint_files = os.listdir('{}/checkpoint'.format(save_dir))\n",
    "    checkpoint_list = [os.path.join('{}/checkpoint'.format(save_dir), name) for name in checkpoint_files]\n",
    "    \n",
    "    if checkpoint_list != [] and cfg.TRAIN.USE_CHECKPOINT:\n",
    "        if specific_checkpoint is not None:\n",
    "            checkpoint_path = os.path.join('{}/checkpoint'.format(save_dir), specific_checkpoint)\n",
    "        else:\n",
    "            checkpoint_path = max(checkpoint_list, key=os.path.getctime)\n",
    "            \n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if not cfg.TRAIN.NEW_RUN:\n",
    "            last_epochs = checkpoint['epoch']\n",
    "            best_acc = checkpoint['best_acc']\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                model_ema.load_state_dict(checkpoint['model_ema_state_dict'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print('Loaded checkpoint: {}\\n'.format(checkpoint_path))\n",
    "        \n",
    "    return last_epochs, best_acc\n",
    "\n",
    "def load_data(model, optimizer=None, scheduler=None, specific_checkpoint=None):\n",
    "    last_epochs = 0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    try:\n",
    "        save_dir = cfg.SAVE_DIR\n",
    "        os.makedirs('{}/checkpoint'.format(save_dir))\n",
    "        os.makedirs('{}/best_acc'.format(save_dir))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if cfg.TRAIN.PRETRAINED is not None:\n",
    "        load_pretrained(model, pretrained_dir=cfg.TRAIN.PRETRAINED)\n",
    "    elif cfg.TRAIN.USE_CHECKPOINT:\n",
    "        last_epochs, best_acc = load_checkpoint(model, optimizer, scheduler)\n",
    "\n",
    "    return last_epochs, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler):\n",
    "    since = time.time()\n",
    "    \n",
    "    gradient_accumulation = cfg.TRAIN.ACCUMULATION_STEPS\n",
    "    save_dir = cfg.SAVE_DIR\n",
    "    last_epochs = 0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    last_epochs, best_acc = load_data(model, optimizer, scheduler)\n",
    "    \n",
    "    if cfg.USE_WANDB:\n",
    "        wandb.watch(model, criterion=criterion_STCE, log='all', log_freq=500)\n",
    "    \n",
    "    print('Training start from {} epoch\\n'.format(last_epochs + 1))\n",
    "    \n",
    "    for epoch in range(last_epochs, cfg.TRAIN.EPOCHS):\n",
    "        print('train start - epoch : {} - lr : {:.4e}\\n'.format(epoch + 1, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        #train\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for idx, (datas, labels) in enumerate(tqdm(train_loader)):\n",
    "            datas = datas.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            \n",
    "            datas, labels = mixup_fn(datas, labels)\n",
    "            \n",
    "            with amp.autocast(enabled=amp_enable):\n",
    "                output,_ = model(datas)\n",
    "                \n",
    "                loss = criterion_STCE(output, labels)\n",
    "                loss = loss / gradient_accumulation\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (idx + 1) % gradient_accumulation == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if cfg.MODEL.USE_EMA:\n",
    "                    model_ema.update(model)\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation / len(train_loader)\n",
    "                \n",
    "            #wandb\n",
    "            if cfg.USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'iteration': idx + epoch * len(train_loader),\n",
    "                    'train loss': loss.item() * gradient_accumulation\n",
    "                })\n",
    "        \n",
    "        print('train end - epoch : {} - loss : {:.4f} - lr : {:.4e}\\n'.format(epoch + 1, epoch_loss, optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        #evaluate\n",
    "        epoch_val_top1 = 0\n",
    "        epoch_val_top2 = 0\n",
    "        epoch_val_top3 = 0\n",
    "        epoch_val_top4 = 0\n",
    "        epoch_val_top5 = 0\n",
    "\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        if cfg.VALID.MODEL_EMA:\n",
    "            model_ema.cuda()\n",
    "            model_ema.eval()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, (datas, labels) in enumerate(tqdm(valid_loader)):\n",
    "                datas = datas.cuda(non_blocking=True)\n",
    "                labels = labels.cuda(non_blocking=True)\n",
    "                \n",
    "                with amp.autocast(enabled=amp_enable):\n",
    "                    if cfg.VALID.MODEL_EMA:\n",
    "                        val_output,_ = model_ema.module(datas)\n",
    "                    else:\n",
    "                        val_output,_ = model(datas)\n",
    "                    \n",
    "                    val_loss = criterion_CE(val_output, labels)\n",
    "\n",
    "                maxk = torch.topk(val_output, 5)[1]\n",
    "                top1 = (maxk[:, 0] == labels).sum() / labels.size(0)\n",
    "                top2 = (maxk[:, 0:2] == labels.view(-1, 1)).sum() / labels.size(0)\n",
    "                top3 = (maxk[:, 0:3] == labels.view(-1, 1)).sum() / labels.size(0)\n",
    "                top4 = (maxk[:, 0:4] == labels.view(-1, 1)).sum() / labels.size(0)\n",
    "                top5 = (maxk == labels.view(-1, 1)).sum() / labels.size(0)\n",
    "\n",
    "                epoch_val_top1 += top1 / len(valid_loader)\n",
    "                epoch_val_top2 += top2 / len(valid_loader)\n",
    "                epoch_val_top3 += top3 / len(valid_loader)\n",
    "                epoch_val_top4 += top4 / len(valid_loader)\n",
    "                epoch_val_top5 += top5 / len(valid_loader)\n",
    "\n",
    "                epoch_val_loss += val_loss.item() / len(valid_loader)\n",
    "            \n",
    "                for lb in labels:\n",
    "                    label_list.append(lb.data.item())\n",
    "\n",
    "                for top1 in maxk[:, 0]:\n",
    "                    pred_list.append(top1.data.item())\n",
    "                \n",
    "        if cfg.VALID.MODEL_EMA:\n",
    "            model_ema.cpu()\n",
    "                    \n",
    "        if cfg.USE_WANDB:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch train loss': epoch_loss,\n",
    "                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                'epoch valid loss': epoch_val_loss,\n",
    "                'epoch valid acc@1': epoch_val_top1,\n",
    "                'epoch valid acc@2': epoch_val_top2,\n",
    "                'epoch valid acc@3': epoch_val_top3,\n",
    "                'epoch valid acc@4': epoch_val_top4,\n",
    "                'epoch valid acc@5': epoch_val_top5,\n",
    "            })\n",
    "        \n",
    "        print(f'validation - epoch : {epoch + 1} - loss : {epoch_val_loss:.4f} - top1: {epoch_val_top1:.4f} - top2: {epoch_val_top2:.4f} - top3 : {epoch_val_top3:.4f} - top4: {epoch_val_top4:.4f} - top5: {epoch_val_top5:.4f}\\n')\n",
    "        print(classification_report(label_list, pred_list, target_names=class_list, digits=4))\n",
    "\n",
    "        #lr schedule\n",
    "        scheduler.step(epoch + 1)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': epoch_loss,\n",
    "            'val_loss': epoch_val_loss,\n",
    "            'val_acc' : epoch_val_top1,\n",
    "            'best_acc' : best_acc,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        }\n",
    "        \n",
    "        if cfg.TRAIN.AMP_ENABLE:\n",
    "            checkpoint['scaler_state_dict'] = scaler.state_dict()\n",
    "        if cfg.MODEL.USE_EMA:\n",
    "            checkpoint['model_ema_state_dict'] = model_ema.state_dict()\n",
    "        \n",
    "        if epoch_val_top1 > best_acc:\n",
    "            best_acc = epoch_val_top1\n",
    "            torch.save(checkpoint, '{}/best_acc/{}_acc_{:.2f}_{}.pth'.format(save_dir, cfg.NAME, best_acc*100, epoch + 1))\n",
    "            print('Best accuracy model saved - epoch: {} - best_acc: {}\\n'.format(epoch + 1, best_acc))\n",
    "        \n",
    "        torch.save(checkpoint, '{}/checkpoint/{}_ckpt_{}.pth'.format(save_dir, cfg.NAME, epoch + 1))\n",
    "           \n",
    "    #print final result\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(time_elapsed // 3600,\n",
    "        (time_elapsed % 3600) // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training and validation embedding (for xgboost ensemble )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(model, data_loader, specific_checkpoint=None):\n",
    "    emb_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    load_data(model, specific_checkpoint=specific_checkpoint)\n",
    "    \n",
    "    if cfg.VALID.MODEL_EMA:\n",
    "        model_ema.cuda()\n",
    "        model_ema.eval()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (datas, labels) in enumerate(tqdm(data_loader)):\n",
    "            datas = datas.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            \n",
    "            with amp.autocast(enabled=amp_enable):\n",
    "                if cfg.VALID.MODEL_EMA:\n",
    "                    _, embs = model_ema.module(datas)\n",
    "                else:\n",
    "                    _, embs = model(datas)\n",
    "            \n",
    "            emb_list.append(embs.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "            \n",
    "        emb_list = torch.cat(emb_list)\n",
    "        label_list = torch.cat(label_list)\n",
    "    \n",
    "    if cfg.VALID.MODEL_EMA:\n",
    "        model_ema.cpu()\n",
    "            \n",
    "    return emb_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_list, train_label_list = get_emb(model, query_loader, specific_checkpoint=None)\n",
    "valid_emb_list, valid_label_list = get_emb(model, valid_loader, specific_checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embedding data array for later ensemble\n",
    "l = [train_emb_list.numpy(), train_label_list.numpy(), valid_emb_list.numpy(), valid_label_list.numpy()]\n",
    "with open(\"train_emb_convnext_base\", \"wb\") as fp:\n",
    "    pickle.dump(l, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get testing embedding (for xgboost ensemble )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = glob.glob(os.path.join(cfg.DATA.PUBLIC_TEST_DIR, \"*\")) + glob.glob(os.path.join(cfg.DATA.PRIVATE_TEST_DIR, \"*\"))\n",
    "test_list = sorted(test_list)\n",
    "\n",
    "print(len(test_list))\n",
    "\n",
    "class AIdeaTestDataset(Dataset):\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_list[idx]).copy()\n",
    "        W, H = img.size\n",
    "        \n",
    "        img = np.array(img)\n",
    "\n",
    "        if cfg.DATA.CROP:\n",
    "            target_x = float(target_df.loc[target_df['filename'] == os.path.split(self.file_list[idx])[1]]['target_x'].to_string(index=False))\n",
    "            target_y = float(target_df.loc[target_df['filename'] == os.path.split(self.file_list[idx])[1]]['target_y'].to_string(index=False))\n",
    "            \n",
    "            if W > H:\n",
    "                if W/2 - H/2 + target_x * W < 0:\n",
    "                    x_min = 0\n",
    "                elif W/2 - H/2 + target_x * W > W - H:\n",
    "                    x_min = int(W - H)\n",
    "                else:\n",
    "                    x_min = int(W/2 - H/2 + target_x * W)\n",
    "                x_max = x_min + H\n",
    "                y_min = 0\n",
    "                y_max = H\n",
    "            else:\n",
    "                x_min = 0\n",
    "                x_max = W\n",
    "                if H/2 - W/2 + target_y * H < 0:\n",
    "                    y_min = 0\n",
    "                elif H/2 - W/2 + target_y * H > H - W:\n",
    "                    y_min = int(H - W)\n",
    "                else:\n",
    "                    y_min = int(H/2 - W/2 + target_y * H)\n",
    "                y_max = y_min + W\n",
    "            \n",
    "            img = A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max)(image=img)['image']\n",
    "        img_transformed = self.transform(image=img)\n",
    "        img_transformed = img_transformed['image']\n",
    "        \n",
    "        return img_transformed, os.path.split(self.file_list[idx])[1]\n",
    "\n",
    "test_data = AIdeaTestDataset(test_list, transform=valid_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=cfg.DATA.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=cfg.DATA.PIN_MEMORY,\n",
    "    num_workers=cfg.DATA.NUM_WORKERS,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emb(model, test_loader, specific_checkpoint):\n",
    "    test_emb_list = []\n",
    "    test_file_list = []\n",
    "    \n",
    "    load_data(model, specific_checkpoint=specific_checkpoint)\n",
    "    \n",
    "    if cfg.VALID.MODEL_EMA:\n",
    "        model_ema.cuda()\n",
    "        model_ema.eval()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for idx, (datas, file_name) in enumerate(tqdm(test_loader)):\n",
    "            datas = datas.cuda(non_blocking=True)\n",
    "            \n",
    "            with amp.autocast(enabled=amp_enable):\n",
    "                if cfg.VALID.MODEL_EMA:\n",
    "                    _, emb = model_ema.module(datas)\n",
    "                else:\n",
    "                    _, emb = model(datas)\n",
    "                    \n",
    "            test_emb_list.append(emb.cpu())\n",
    "            test_file_list += file_name\n",
    "            \n",
    "    if cfg.VALID.MODEL_EMA:\n",
    "        model_ema.cpu()\n",
    "        \n",
    "    return torch.cat(test_emb_list), test_file_list\n",
    "\n",
    "test_emb_list, test_file_list = test_emb(model, test_loader, specific_checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embedding data array for later ensemble\n",
    "l = [test_emb_list.numpy(), test_file_list]\n",
    "with open(\"test_emb_convnext_base\", \"wb\") as fp:\n",
    "    pickle.dump(l, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c29844a0a6289b07ec9bd49bfbf82b25be0a16fcb62431206df354e3cda954d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
